substitutions:
  _DATAFLOW_JAR_BUCKET: "${PROJECT_ID}-artifacts"
  _DATAFLOW_STAGING_BUCKET: "${PROJECT_ID}-artifacts-staging"
  _COMPOSER_REGION: 'us-central1'
  _COMPOSER_ZONE_ID: 'us-central1-a'
  _COMPOSER_ENV_NAME: 'dlp-pipeline-composer'
  _COMPOSER_DAG_NAME: 'db-import'
timeout: 1200s
steps:
  - name: gcr.io/cloud-builders/git
    args: [ 'clone', 'https://github.com/krishnachaitanya-v/dlp-rdb-bq-import.git' ]
    id: 'clone-source-code'
  - name: gcr.io/cloud-builders/gradle:5.6.2-jdk-8
    args: [ 'assemble' ]
    dir: '$REPO_NAME'
    id: 'build-jar'
  - name: gcr.io/cloud-builders/gsutil
    args: [ 'cp', '*.jar', 'gs://${_DATAFLOW_JAR_BUCKET}/' ]
    dir: '$REPO_NAME/build/libs'
    id: 'deploy-jar'
  - name: gcr.io/cloud-builders/gcloud
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo { \"gcp_project\":\"${PROJECT_ID}\", \
        \"gcp_region\":\"${_COMPOSER_REGION}\", \
        \"gcp_zone\":\"${_COMPOSER_ZONE_ID}\", \
        \"dataflow_jar_location\":\"${_DATAFLOW_JAR_BUCKET}\", \
        \"dataflow_jar_file\":\"dlp-rdb-bq-import-dataflow.jar\", \
        \"dataflow_staging_bucket\":\"${_DATAFLOW_STAGING_BUCKET}\"} > /workspace/composer_variables.json && \
        cat /workspace/composer_variables.json
    id: 'prepare-composer-env-json'
  - name: gcr.io/cloud-builders/gcloud
    args: [ 'composer', 'environments', 'storage', 'data', 'import', '--environment', '${_COMPOSER_ENV_NAME}', '--location', '${_COMPOSER_REGION}', '--source', '/workspace/composer_variables.json' ]
    dir: '$REPO_NAME/env-setup'
    id: 'copy-composer-env-json'
  - name: gcr.io/cloud-builders/gcloud
    args: [ 'composer', 'environments', 'run', '${_COMPOSER_ENV_NAME}', '--location', '${_COMPOSER_REGION}', 'variables', '--', '--i', '/home/airflow/gcs/data/composer_variables.json' ]
    id: 'setup-composer-environment'
  - name: gcr.io/cloud-builders/gcloud
    args: [ 'composer', 'environments', 'storage', 'dags', 'import', '--environment', '${_COMPOSER_ENV_NAME}', '--location', '${_COMPOSER_REGION}', '--source', 'data_pipeline.py' ]
    dir: '$REPO_NAME/workflow-dags'
    id: 'deploy-processing-pipeline'
  - name: gcr.io/cloud-builders/gcloud
    entrypoint: 'bash'
    args: [ './wait_for_dag_deployed.sh', '${_COMPOSER_ENV_NAME}', '${_COMPOSER_REGION}', '${_COMPOSER_DAG_NAME}', '6', '20' ]
    dir: '$REPO_NAME/build-pipeline'
    id: 'wait-for-dag-deployed-on-composer'
  - name: gcr.io/cloud-builders/gcloud
    args: [ 'composer', 'environments', 'run', '${_COMPOSER_ENV_NAME}', '--location', '${_COMPOSER_REGION}', 'trigger_dag', '--', '${_COMPOSER_DAG_NAME}', '--run_id=$BUILD_ID' ]
    id: 'trigger-pipeline-execution'
